---
title: "Beijing Airbnb Pricing & Rating Predictions "
author: "Kobi Ocran"
date: "5/8/2020"
output:
  html_document:
    code_folding: hide
---

```{r setup, include = TRUE }
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

```{r}
library(ggfortify)

library(corrplot)

library(ggplot2)

library(dplyr)

library(plotly)

library(leaflet)

library(rpart.plot)

library(caret)

library(kableExtra)

library(knitr)

library(mvtnorm)
```

```{r}
# Read in data
TrainingListings = read.csv("/Users/kobiocran/Desktop/DA350/Computational Final /TrainingListings.csv")

TestListings = read.csv("/Users/kobiocran/Desktop/DA350/Computational Final /TestListings.csv")
```

# Task 1 : Patterns and Trends

### This section will explore the relationship predictors and property listings

```{r}
#Data Cleaning & Exploration

rownames(TrainingListings) <- TrainingListings$id

#Bathrooms are in decimals. Must be converted to whole numbers
TrainingListings$bathrooms <- round(c(TrainingListings$bathrooms), digits = 0) 

TestListings$bathrooms <- round(c(TestListings$bathrooms), digits = 0)

#Removing Outliers. The owner cannot profit from free rent.In addition, $50,000 per night seems to extravagant.
TrainingListings <- TrainingListings %>% 
  filter( price != 0 & price < 50000)

TrainingListings %>% ggplot(aes(x = price)) +
  geom_histogram(bins = 30) +
  labs(title = "Histogram of AirBnb Price Listings") +
  theme(plot.title = element_text(hjust = 0.5))
  
  
```

The original data included listings that had no price tag. These needed to be remove. Owners cannot profit from unpaid rent. The histogram is heavily skewed to the left as most properties cost under 5000 Chinese yuan per night

## Unsupervised Method 1: PCA

```{r}
# Correlation Plot of the review scores
corplot <- autoplot(cor(TrainingListings[,10:16]), main = "Correlation Plot for Various Review Scores")

corplot + theme(axis.text.x = element_text(angle = 60, hjust = 1))
```

The review scores for cleanliness is only slightly correlated to the review scores for location. Review scores ratings is highly positively correlated to review scores for accuracy, cleanliness and value. Review scores for communication is also highly correlated to review scores for checking. 

```{r}
#group by propert
property_type_data <- TrainingListings %>%
  group_by(property_type) %>%
  summarise(avg_accomodates = mean(accommodates), avg_bathrooms = mean(bathrooms), avg_bedrooms = mean(bedrooms), avg_review_scores_rating = mean(review_scores_rating), avg_review_scores_accuracy = mean(review_scores_accuracy), avg_review_scores_cleanliness = mean(review_scores_cleanliness), avg_review_scores_checkin = mean(review_scores_checkin), avg_review_scores_communication = mean(review_scores_communication), avg_review_scores_location = mean(review_scores_location), avg_review_scores_value = mean(review_scores_value), avg_host_listings_count = mean(host_listings_count))

write.csv(property_type_data,'property_type_data.csv')

property_type_data = read.csv("/Users/kobiocran/Desktop/DA350/property_type_data.csv", row.names = 2)
  
review_scores.pca = prcomp(property_type_data[,c(2:11)], scale. = TRUE)

# Consider first three components and perform biplot to identify patterns
round(review_scores.pca$rotation[,1:3],5)

autoplot(review_scores.pca, loadings = TRUE, loadings.colour = 'blue', label = TRUE, shape = FALSE, loadings.label = TRUE, loadings.label.size = 3, main = "Principal Component Analysis of Ratings & Amenities")
```

The first principal component and second principal component are seen to have large posititve associations with properties that are highly rated for property locations. Properties with negative PC1 are normally farm Stays and villas. Villas offer more amenities and will tend to be more expensive compared to a condominium, a hotel, a house or an apartment. 

```{r}
# Lets us explore PCA for advertising and how it may influence people's ratings of listings
review_scores.pca3 = prcomp(property_type_data[,c(7,8,10,11,12)],scale. = TRUE)

round(review_scores.pca3$rotation[,1:3],5)

autoplot(review_scores.pca3, loadings = TRUE, loadings.colour = 'green', label = TRUE, shape = FALSE, loadings.label = TRUE, loadings.label.size = 3, main = "Principal Component Analysis for Host Lisitngs & Selected Ratings")

```

Properties such as hotels, apartment and house tend to be advertised more, and in this case, have large negative PC1's and large positive PC2's. Villas and Farm Stays are often pristine and known for their high property value thus have large positive associations with review scores value and cleanilness. Addtionally, it might be easier to check in these properties due to low demand. Hotels, Apartments and Houses are often occupied and may make it difficult for a visitor to secure an affordable listing.


```{r}
# Let us consider only review scores from the orginal data only
review_scores.pca2 = prcomp(TrainingListings[,c(10:16)],scale. = TRUE)
round(review_scores.pca2$rotation[,1:3],5)

autoplot(review_scores.pca2, loadings = TRUE, loadings.colour = 'red', label = TRUE, shape = FALSE, loadings.label = TRUE, loadings.label.size = 3, main = "Principal Component Analysis for Selected Ratings")
```

The PCA plot above contains the property ID. It is quite more cumbersome but useful in generating prive estimates when implementing PCA on a regression model for each Airbnb listing on the test data (unobserved prices of lisitings).


```{r}
# Convert property_type and room_type to dummy variables (Training Data)

# property_types

TrainingListings <- TrainingListings%>%
  mutate(Apartment = ifelse(property_type == 'Apartment',1,0))

TrainingListings <- TrainingListings%>%
  mutate(Condominium = ifelse(property_type == 'Condominium',1,0))

TrainingListings <- TrainingListings%>%
  mutate(`Farm stay` = ifelse(property_type == 'Farm stay',1,0))

TrainingListings <- TrainingListings%>%
  mutate(Hotel = ifelse(property_type == 'Hotel',1,0))

TrainingListings <- TrainingListings%>%
  mutate(House = ifelse(property_type == 'House',1,0))

TrainingListings <- TrainingListings%>%
  mutate(Villa = ifelse(property_type == 'Villa',1,0))

# room_type

TrainingListings <- TrainingListings%>%
  mutate(`Entire home/apt` = ifelse(property_type == 'Entire home/apt',1,0))

TrainingListings <- TrainingListings%>%
  mutate(`Private room` = ifelse(property_type == 'Private room',1,0))

TrainingListings <- TrainingListings%>%
  mutate(`Shared room` = ifelse(property_type == 'Share room',1,0))

# Convert property_type and room_type to dummy variables (Test Data)

# property_types

TestListings <- TestListings%>%
  mutate(Apartment = ifelse(property_type == 'Apartment',1,0))

TestListings <- TestListings%>%
  mutate(Condominium = ifelse(property_type == 'Condominium',1,0))

TestListings <- TestListings%>%
  mutate(`Farm stay` = ifelse(property_type == 'Farm stay',1,0))

TestListings <- TestListings%>%
  mutate(Hotel = ifelse(property_type == 'Hotel',1,0))

TestListings <- TestListings%>%
  mutate(House = ifelse(property_type == 'House',1,0))

TestListings <- TestListings%>%
  mutate(Villa = ifelse(property_type == 'Villa',1,0))

# room_type

TestListings <- TestListings%>%
  mutate(`Entire home/apt` = ifelse(property_type == 'Entire home/apt',1,0))

TestListings <- TestListings%>%
  mutate(`Private room` = ifelse(property_type == 'Private room',1,0))

TestListings <- TestListings%>%
  mutate(`Shared room` = ifelse(property_type == 'Share room',1,0))

```


```{r}
# Color by property type / room_type
TrainingListings$PC1 = review_scores.pca2$x[,1]
TrainingListings$PC2 = review_scores.pca2$x[,2]
TrainingListings$PC3 = review_scores.pca2$x[,3]

property_type_function_pca2 = function(my_property_type){plot_ly(TrainingListings, x = ~PC1, y = ~PC2, z = ~PC3, color= ~ my_property_type) %>%
  add_markers() %>%
  layout(scene = list(xaxis = list(title = 'PCA1'),
                      yaxis = list(title = 'PCA2'),
                      zaxis = list(title = 'PCA3')))
}

property_type_function_pca2(TrainingListings$room_type)

property_type_function_pca2(TrainingListings$property_type)

# reorder columns
TrainingListings <- subset(TrainingListings, select = c(1:16,19:30,17,18))
```

The 3-D plots indicate the Prinicpal Components for each room and property type

## Unsupervised Method 2: Clustering of Amenities 

```{r}
### Unsupervised Method 2: Clustering of Amenities (INCOMPLETE INTERPRETATION)

wss = list(15)
for (k in 2:15){ 
  wss[k] = sum(kmeans(TrainingListings[,c(7,8,9)], k, nstart=10)$tot.withinss)
}
plot(2:15, wss[2:15], type="b", xlab="Number of Clusters",
     ylab="Within groups sum of squares")

FiveFit = kmeans(TrainingListings[,c(7,8,9)], 5, nstart=50)

plot_ly(TrainingListings, x = ~accommodates, y = ~bathrooms, z = ~bedrooms, color = ~FiveFit$cluster) %>%
  add_markers() %>%
  layout(scene = list(xaxis = list(title = 'accommodates'),
                      yaxis = list(title = 'bathrooms'),
                      zaxis = list(title = 'bedrooms')))

```

Clustering is helpful indentifying the relationship of properties and their amenities. According to the plot, A lot properties have a high combination of accomodates and bathrooms.


# Task 2 : Predictions on Rental Price (Regression Predicitons)

In this section will will incorpate Principal Component Analysis and K-means Clustering in regression to predict prices in the test data. Support Vector Machines is implemented as well. 

```{r}

###Filter Training Dataset with all predictors and price
TrainingListingsPrice <- TrainingListings %>%
  select(host_listings_count, accommodates, bathrooms, bedrooms,review_scores_rating:review_scores_value,Apartment:`Shared room`,price)

#Perform Stepwise Regression to identify relevant variables for Rental Price
null_price <- glm(price ~ 1, data = TrainingListingsPrice)
full_price <- glm(price ~ ., data = TrainingListingsPrice)
step(null_price,scope = list(lower = null_price, upper = full_price), direction = "forward")

TrainingListingsPrice["id"] <- TrainingListings[,1]

#reorder columns
TrainingListingsPrice <- TrainingListingsPrice[c(22,1:21)]
# The variables that seem to be the most significant variables in prediicting price according to the stepwise regression are indicated in the following:
### accommodates, 
### bedrooms
#Villa
#House, 
#Hotel
#review_scores_rating
#review_scores_value
#review_scores_accuracy
#`Farm stay`
#review_scores_checkin
#review_scores_cleanliness
#review_scores_location
```

According to the Stepwise Regression, the variables deemed significant in affect price are shown below:

- accommodates
- bedrooms
- Villa
- House 
- Hotel
- review_scores_rating
- review_scores_value
- review_scores_accuracy
- Farm stay
- review_scores_checkin
- review_scores_cleanliness
- review_scores_location

## Method 1: PCA in Regression

```{r}
pca_var = review_scores.pca2$sdev^2
prop_varex <- pca_var/sum(pca_var) #Proportion of variance

plot(prop_varex,type='b')
plot(cumsum(prop_varex),type = 'b')
which.max(cumsum(prop_varex)>.90)
```

4 components are needed to explain 90% of the variability in the data.

```{r}
### Linear Regression
ctrl = trainControl(method="repeatedcv",number = 10, repeats = 5) #pcaComp = 2:18 indicates significant predictors
lm_fit1 = train(price ~ accommodates + bathrooms + as.factor(Villa) + bedrooms + as.factor(House) + as.factor(Hotel) + review_scores_rating + review_scores_value + review_scores_accuracy + as.factor(`Farm stay`) + review_scores_checkin + review_scores_cleanliness + review_scores_location,
                  data = TrainingListingsPrice,
                  method = "lm", 
                  trControl = ctrl)

#lm_predictions = predict(lim_fit1,TestListings)

### Add 4 components that explain 90%
TrainingListingsPrice[,c("PC1","PC2","PC3","PC4")] = review_scores.pca2$x
TestListings[,c("PC1","PC2","PC3","PC4")] = review_scores.pca2$x

pca_fit_a = train(price ~ PC1, 
                data = TrainingListingsPrice, 
                method = "lm", 
                trControl = ctrl)

pca_fit_a$results$RMSE

pca_fit_b = train(price ~ PC1 + PC2 , 
                data = TrainingListingsPrice, 
                method = "lm", 
                trControl = ctrl)

pca_fit_b$results$RMSE

pca_fit_c = train(price ~ PC1 + PC2 + PC3, 
                data = TrainingListingsPrice, 
                method = "lm", 
                trControl = ctrl)

pca_fit_c$results$RMSE


pca_fit1 = train(price ~ PC1 + PC2 + PC3 + PC4, 
                data = TrainingListingsPrice, 
                method = "lm", 
                trControl = ctrl)

pca_fit1$results$RMSE


#Generate Estimates
pca_fit1predictions = round(predict(pca_fit1,TestListings))

```
 
There is a gradual decrease of the RMSE once each component is added for each step. 

## Method 2: K-Nearest Neigbours in Regression

```{r}
#Removing PCA Data from previous method 
TrainingListingsPrice <- TrainingListings %>%
  select(host_listings_count, accommodates, bathrooms, bedrooms,review_scores_rating:review_scores_value,Apartment:`Shared room`,price)



TrainingListingsPrice["id"] <- TrainingListings[,1]

#reorder columns
TrainingListingsPrice <- TrainingListingsPrice[c(22,1:21)]

trctrl = trainControl(method = "repeatedcv", number = 8, repeats = 3)


regFFit_trial = train(price ~ . ,
                 data = TrainingListingsPrice,
                 trControl = trctrl,
                 method = "knn",
                 tuneGrid=expand.grid(k=c(5,7,9,11,13))) #This helps test with various k values and picks the optimal k.

#Generate Estimates
knn_predictions =  predict(regFFit_trial,TestListings)
```

## Method 3: Support Vector Machines 

```{r}
### Method 3: Support Vector Machines

set.seed(1)

trctrl = trainControl("cv",number = 2)

support_vector_model_1 <- train(price ~ .,
                                data = TrainingListingsPrice, 
                                method = "svmLinear",
                                trControl = trctrl,
                                tuneGrid = expand.grid(C = seq(0, 2, length = 5))) 
                                
                                
support_vector_predictions <- predict(support_vector_model_1,TestListings)

support_vector_predictions = data.frame(support_vector_predictions)

Test_Data1 <- cbind(TestListings,support_vector_predictions)

tail(Test_Data1) %>% kable() %>% kable_styling()
```

```{r}

#Suggested Model
knn_predictions <- data.frame(knn_predictions)

TestListings <- cbind(TestListings,knn_predictions)


```

## Suggested Model: K Nearest Neighbors

K - nearest Neighbors would be an appropriate method for predicting the prices of each Airbnb lisitings. The PCA model is not effective because of the low number of components that are required to explain the best part of the data. The model also suffers from an increase once all components are considered. The support Vector Machine machines displays negative prices and hence higly inappropriate for predictions. K - nearest Neighbors does not assume probability distribution making it highly effective for making predictions.

## Predictor Variables and Predicted Price Insights

```{r}
#Let us explore some the relationships between price and the top three significant variables from the stepwise regression

Accommodates_Data <- TestListings %>%
  group_by(accommodates)%>%
  summarise(avg_price = mean(knn_predictions))

Accommodates_Data %>%
  ggplot(aes(accommodates,avg_price)) +
  geom_point()+
  labs(title = "Price and Accomodates" ,x= "accomodates", y= "price") +
  theme(plot.title = element_text(hjust = 0.5))
  
```

There seems to be a steady relation between accomodates and average price 

```{r}
bedrooms_Data <- TestListings %>%
  group_by(bedrooms)%>%
  summarise(avg_price = mean(knn_predictions))

bedrooms_Data %>%
  ggplot(aes(bedrooms, avg_price)) +
  geom_point()+
  labs(title = "Price and Bedrooms" ,x= "bedrooms", y= "price") +
  theme(plot.title = element_text(hjust = 0.5))
```

There is a positive relationship between the number of bedrooms and price. This seems practical and reasonable

```{r}
Villa_data <- TestListings
Villa_data$Villa <-as.factor(Villa_data$Villa)

Villa_data %>%
  ggplot(aes(x = Villa, y = knn_predictions))+
  geom_boxplot(outlier.colour="black", outlier.shape=16,
             outlier.size=2, notch=FALSE)+
  labs(title = "Price and Villas" ,x= "Villa", y= "price") +
  theme(plot.title = element_text(hjust = 0.5))
  
```

A property that is a villa is likely to be more expensive than one that is not. The boxplots also seem distinct for each other making Villa a good predicitor for the price of an Airbnb lisiting.


# Task 3: Predicitions on Ratings (Classification Predictions)

```{r}
#Q.3) Predictions on Many Ratings ('Classification Predictions')

###Filter Training Dataset with predictors and ManyRatings
TrainingListingsMR <- TrainingListings %>%
  select(host_listings_count, accommodates, bathrooms, bedrooms,review_scores_rating:review_scores_value,Apartment:`Shared room`,ManyReviews)

###Perform Stepwise Regression to identify relevant variables for Many Ratings 
null_mr <- glm(ManyReviews ~ 1, data = TrainingListingsMR)
full_mr <- glm(ManyReviews ~ ., data = TrainingListingsMR)
step(null_mr,scope = list(lower = null_mr, upper = full_mr), direction = "forward")

TrainingListingsMR["id"] <- TrainingListings[,1]

#reorder columns
TrainingListingsMR <- TrainingListingsMR[c(22,1:21)]

### According to the stepwise regression model the variables that are significant are : 
#review_scores_location 
#bedrooms
#review_scores_Accuracy 
#Farm Stay
#host_lisitngs_count 
#Condominium
#Villa
#review_scores_checkin 
#hotel
#accomodates 
#review_scores_rating 
#review_scores_cleanliness 
#review_scores_value
#Apartment 
```

According to the stepwise regression model the variables that are significant are : 
 - review_scores_location 
 - bedrooms
 - review_scores_Accuracy 
 - Farm Stay
 - host_lisitngs_count 
 - Condominium
 - Villa
 -  review_scores_checkin 
 -  hotel
 -  accomodates 
 -  review_scores_rating 
 - review_scores_cleanliness 
 - review_scores_value
 - Apartment
 
###Method 1: Linear Discriminant Analysis

```{r}
###Calculate value of mu 

total_listings = as.numeric(nrow(TrainingListingsMR))

#There 15234 Airbnb Lisitings

listings_mr <- TrainingListingsMR %>%
  filter(ManyReviews == 1) %>%
  count() %>%
  as.numeric()

# Of these 15234 lisitngs, 6386 have received more than 50 reviews. 

mu_mr <- listings_mr/total_listings ; mu_mr

listings_zero <- TrainingListingsMR %>%
  filter(ManyReviews == 0) %>%
  count() %>%
  as.numeric()

mu_zero <- listings_zero/total_listings ; mu_zero

# Of these 15234 lisitngs, 8848  have received less than 50 reviews.

#The probability for a lisiting to receive many reviews is 0.4191939

#The probability for a lisiting to not receive many reviews is 0.5808061

### all factor variables were omitted
TrainingListingsMR_subset <- TrainingListingsMR %>%
  select(review_scores_location, bedrooms, review_scores_accuracy, host_listings_count, review_scores_checkin, accommodates, review_scores_rating, review_scores_cleanliness, review_scores_value, ManyReviews)

#scale function standardizes the dataset
TrainingListingsMR_subset <- scale(TrainingListingsMR_subset[1:9], center = TRUE, scale = TRUE)

TrainingListingsMR_subset <- data.frame(TrainingListingsMR_subset)

TrainingListingsMR_subset['ManyReviews'] <- TrainingListingsMR[,22]

#Calculate mean vector
get_mean_vectors <- function(many_reviews){
  
  mean_vectors = c()

#subset the training data by Listing with ManyReviews
  temp <- 
    TrainingListingsMR_subset%>%
    filter(ManyReviews == many_reviews)


#find the mean for each column, based on the specified genre 
  for(a in 1:9){
    m = mean(temp[, a])
    mean_vectors = append(mean_vectors, m) 
  }
  
  return(mean_vectors)
}

mr_mean_vec = get_mean_vectors(1)
zero_mean_vec = get_mean_vectors(0)

#Calculate sigma LDA
cov_LDA = cov(TrainingListingsMR_subset[,1:9])

#Calculate sigma QDA
temp <- 
  TrainingListingsMR_subset%>%
  filter(ManyReviews == 1)

sig_mr = cov(temp[,1:9])

temp <- 
  TrainingListingsMR_subset%>%
  filter(ManyReviews == 0)

sig_zero = cov(temp[,1:9])

#Probability using Bayes

get_prob <- function(f_k, mu_k, f_1, mu_1){
  numerator = f_k * mu_k 
  denomenator = (f_k * mu_k) + (f_1 * mu_1)
  prob = numerator / denomenator
  
  return(prob) 

}

LDA <- function(new_many_ratings){
  
#subset the testing data to include only the significant predictors 
new_many_ratings = data.frame(new_many_ratings$review_scores_location,new_many_ratings$bedrooms,new_many_ratings$review_scores_accuracy,new_many_ratings$host_listings_count,new_many_ratings$review_scores_checkin,new_many_ratings$accommodates,new_many_ratings$review_scores_rating,new_many_ratings$review_scores_cleanliness,new_many_ratings$review_scores_value)
  
  
  #creates a vector that will keep track of the number of correct predictions we make 
  #correct_predictions = c() 
  Classification_Predicitions = c() 
  
  for(index in 1:as.numeric(nrow(new_many_ratings))){
    
    #create a vector that will contain the probability the AirBnb listing has many reviews 
    probs = c() 
    
    many_reviews_features = as.numeric(new_many_ratings[index, 1:9])
    
    #pdf of Many Reviews = 1  
    pdf_mr = dmvnorm(many_reviews_features, mr_mean_vec, cov_LDA)
    
    #pdf of Many Reviews = 0
    pdf_zero = dmvnorm(many_reviews_features, zero_mean_vec , cov_LDA)
    
    #probability Airbnb listing has many reviews. 
    prob_mr= get_prob(pdf_mr, mu_mr, pdf_zero, mu_zero)
    probs = append(probs, prob_mr)
    
    #probability Airbnb lisitng does not have many reviews
    prob_zero = get_prob(pdf_zero, mu_zero, pdf_mr, mu_mr)
    probs = append(probs, prob_zero)
    
    #find the index of the max value
    max_index = which.max(probs)
    
    if(is.na(probs[1]) == TRUE){my_pred = "Other"}
    
    else if(max_index == 1){`Classification Predictions` = append(Classification_Predicitions, 1)}
    else if(max_index == 2){`Classification Predicitions` = append(Classification_Predicitions, 0)}
    
  }
  
  
  
  df1 <-
    data.frame(new_many_ratings[,1:9], `Classification Predicitions`)
  
}


LDA_Airbnb = LDA(TestListings)

LDA_Airbnb['id'] <-TestListings[1]

LDA_Airbnb <- LDA_Airbnb[c(11,1:10)]

head(LDA_Airbnb) %>% kable() %>% kable_styling()
```

LDA is not effective in this case as it predicts every Airbnb Lisiting not to have a high rating, as seen in the table above. We must proceed with a more powerful and effective method

## Method 2: Random Forest

```{r}
### Attempt with logistic model first
ctrl = trainControl(method = "cv")

logistic_model = train(as.factor(ManyReviews) ~ review_scores_location + bedrooms + review_scores_accuracy + as.factor(`Farm stay`) + host_listings_count + as.factor(Condominium) + as.factor(Villa) + review_scores_checkin + as.factor(Hotel) + accommodates + review_scores_rating + review_scores_cleanliness + review_scores_value + as.factor(Apartment),
                       data = TrainingListingsMR, 
                       method = "glm",
                       family = "binomial",
                       trControl = ctrl) ; logistic_model

summary(logistic_model)  

###Proceed with Random Forest Model 

### Omit Predictors of type factor 

#No Pruning
tuneGrid = expand.grid(cp=0)
ctrl = trainControl(method="repeatedcv",number=10,repeats=3)

tree_fit = train(as.factor(ManyReviews) ~ review_scores_location + bedrooms + review_scores_accuracy + host_listings_count + review_scores_checkin + accommodates + review_scores_rating + review_scores_cleanliness + review_scores_value,
                       data = TrainingListingsMR, 
                       method = "rpart",
                       tuneGrid = tuneGrid,
                       trControl = ctrl) ; tree_fit

prp(tree_fit$finalModel)

#With Pruning
tree_fit_pruning_func = function(my_tuneLength){tree_fit_pruning = train(as.factor(ManyReviews) ~ review_scores_location + bedrooms + review_scores_accuracy + host_listings_count + review_scores_checkin + accommodates + review_scores_rating + review_scores_cleanliness + review_scores_value, 
                          data = TrainingListingsMR, 
                          method = "rpart",
                          tuneLength = my_tuneLength,
                          trControl = ctrl) ; tree_fit_pruning
plot(tree_fit_pruning)
prp(tree_fit_pruning$finalModel)

}

tree_fit_pruning_func(20)

tree_fit_pruning_func(30)

tree_fit_pruning_func(40)

tree_fit_pruning_func(50)

tree_fit_pruning_func(100)

tree_fit_pruning_func(120)

# Testing Various Parameters for Random Forest

trctrl = trainControl(method = "oob")

random_forest_model_1 = train(as.factor(ManyReviews) ~ review_scores_location + bedrooms + review_scores_accuracy + host_listings_count + review_scores_checkin + accommodates + review_scores_rating + review_scores_cleanliness + review_scores_value,
                            data = TrainingListingsMR, 
                            method = "rf",
                            trControl = trctrl,
                            tuneLength = 8 , 
                            ntree = 200,
                            importance = TRUE) ; random_forest_model_1

random_forest_model_2 = train(as.factor(ManyReviews) ~ review_scores_location + bedrooms + review_scores_accuracy + host_listings_count + review_scores_checkin + accommodates + review_scores_rating + review_scores_cleanliness + review_scores_value,
                              data = TrainingListingsMR, 
                              method = "rf",
                              trControl = trctrl,
                              tuneLength = 5 , 
                              ntree = 200,
                              importance = TRUE) ; random_forest_model_2


random_forest_model_3 = train(as.factor(ManyReviews) ~ review_scores_location + bedrooms + review_scores_accuracy + host_listings_count + review_scores_checkin + accommodates + review_scores_rating + review_scores_cleanliness + review_scores_value,
                              data = TrainingListingsMR, 
                              method = "rf",
                              trControl = trctrl,
                              tuneLength = 3, 
                              ntree = 200,
                              importance = TRUE) ; random_forest_model_3

random_forest_model_4 = train(as.factor(ManyReviews) ~ review_scores_location + bedrooms + review_scores_accuracy + host_listings_count + review_scores_checkin + accommodates + review_scores_rating + review_scores_cleanliness + review_scores_value,
                              data = TrainingListingsMR, 
                              method = "rf",
                              trControl = trctrl,
                              tuneLength = 3, 
                              ntree = 100,
                              importance = TRUE) ; random_forest_model_4

random_forest_model_5 = train(as.factor(ManyReviews) ~ review_scores_location + bedrooms + review_scores_accuracy + host_listings_count + review_scores_checkin + accommodates + review_scores_rating + review_scores_cleanliness + review_scores_value,
                              data = TrainingListingsMR, 
                              method = "rf",
                              trControl = trctrl,
                              tuneLength = 3, 
                              ntree = 200,
                              importance = TRUE) ; random_forest_model_5

random_forest_model_6 = train(as.factor(ManyReviews) ~ review_scores_location + bedrooms + review_scores_accuracy + host_listings_count + review_scores_checkin + accommodates + review_scores_rating + review_scores_cleanliness + review_scores_value,
                              data = TrainingListingsMR, 
                              method = "rf",
                              trControl = trctrl,
                              tuneLength = 3, 
                              ntree = 250,
                              importance = TRUE) ; random_forest_model_6

random_forest_model_7 = train(as.factor(ManyReviews) ~ review_scores_location + bedrooms + review_scores_accuracy + host_listings_count + review_scores_checkin + accommodates + review_scores_rating + review_scores_cleanliness + review_scores_value,
                              data = TrainingListingsMR, 
                              method = "rf",
                              trControl = trctrl,
                              tuneLength = 3, 
                              ntree = 300,
                              importance = TRUE) ; random_forest_model_7

random_forest_model_8 = train(as.factor(ManyReviews) ~ review_scores_location + bedrooms + review_scores_accuracy + host_listings_count + review_scores_checkin + accommodates + review_scores_rating + review_scores_cleanliness + review_scores_value,
                              data = TrainingListingsMR, 
                              method = "rf",
                              trControl = trctrl,
                              tuneLength = 4, 
                              ntree = 100,
                              importance = TRUE) ; random_forest_model_8

random_forest_model_9 = train(as.factor(ManyReviews) ~ review_scores_location + bedrooms + review_scores_accuracy + host_listings_count + review_scores_checkin + accommodates + review_scores_rating + review_scores_cleanliness + review_scores_value,
                              data = TrainingListingsMR, 
                              method = "rf",
                              trControl = trctrl,
                              tuneLength = 4, 
                              ntree = 200,
                              importance = TRUE) ; random_forest_model_9


random_forest_model_10 = train(as.factor(ManyReviews) ~ review_scores_location + bedrooms + review_scores_accuracy + host_listings_count + review_scores_checkin + accommodates + review_scores_rating + review_scores_cleanliness + review_scores_value,
                              data = TrainingListingsMR, 
                              method = "rf",
                              trControl = trctrl,
                              tuneLength = 4, 
                              ntree = 250,
                              importance = TRUE) ; random_forest_model_10

# The accuracies and kappa values are similar across all 10 random forest models. The predictions that each will generate is likely to be the same


random_forest_predictions_5 = predict(random_forest_model_5,TestListings)

# The accuracy and kappa values are significantly improved when compared to that of the logistic regression model and Linear Discriminant Analysis. The downside to implementing this model is its tendency to overplot. 
```

## Predictor Variables and Predicted Rating Insights

```{r}
random_forest_predictions_5 <- data.frame(random_forest_predictions_5)

TestListings <- cbind(TestListings,random_forest_predictions_5)
```


```{r}
# Merge best estimated with test data

colnames(TestListings)[31] <- "Classification Predictions"

colnames(TestListings)[30] <- "Regression Predictions"

#Only keep orginal predictors 
TestListings <- TestListings%>%
  select(-17:-29)

write.csv(TestListings,'/Users/kobiocran/Desktop/DA350/Predictions.csv',row.names = FALSE)
```


### Suggested Model : Random Forest

In testing various parameters of the random forest, the accuracy had an estimate of 83% and a decent kappa value of about 0.64. This is a good result because it indicates that the estimates are near perfect predictions. 







